{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('../fake_news_data/train.csv')\n",
    "column_n = ['id', 'title', 'author', 'text', 'label']\n",
    "remove_c = ['id','author']\n",
    "categorical_features = []\n",
    "target_col = ['label']\n",
    "text_f = ['title', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import Counter\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stopwords_dict = Counter(stop_words)\n",
    "\n",
    "# remove unused columns\n",
    "def remove_unused_c(df, column_n=remove_c):\n",
    "    df = df.drop(column_n, axis=1)\n",
    "    return df\n",
    "\n",
    "# impute null values with none\n",
    "def null_process(feature_df):\n",
    "    for col in text_f:\n",
    "        feature_df.loc[feature_df[col].isnull(),col] = \"None\"\n",
    "    return feature_df\n",
    "\n",
    "# clean_data\n",
    "def clean_dataset(df):\n",
    "    # remove unused column\n",
    "    df = remove_unused_c(df)    \n",
    "    #impute null value\n",
    "    df = null_process(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Cleaning text from unused characters\n",
    "def clean_text(text):\n",
    "    text = str(text).replace(r'http[\\w:/\\.]+', ' ')  # removing urls\n",
    "    text = str(text).replace(r'[^\\.\\w\\s]', ' ')  # remove everything but characters and punctuation\n",
    "    text = str(text).replace('[^a-zA-Z]', ' ')\n",
    "    text = str(text).replace(r'\\s\\s+', ' ')\n",
    "    text = text.lower().strip()\n",
    "    #text = ' '.join(text)    \n",
    "    return text\n",
    "\n",
    "## Nltk Preprocessing include:\n",
    "# Stop words, Stemming and Lemmetization\n",
    "# For our project we use only Stop word removal\n",
    "def nltk_preprocess(text):\n",
    "    text = clean_text(text)\n",
    "    wordlist = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    text = ' '.join([wnl.lemmatize(word) for word in wordlist if word not in stopwords_dict])\n",
    "    return  text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_dataset(data)\n",
    "df['text'] = df.text.apply(nltk_preprocess)\n",
    "df['title'] = df.title.apply(nltk_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
    "    installed).\n",
    "\n",
    "    Args:\n",
    "        seed (:obj:`int`): The seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if is_torch_available():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # ^^ safe to call this function even if cuda is not available\n",
    "    if is_tf_available():\n",
    "        import tensorflow as tf\n",
    "\n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "max_length= 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 14.0kB/s]\n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 425kB/s]  \n",
      "Downloading: 100%|██████████| 455k/455k [00:00<00:00, 524kB/s]  \n",
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 285kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preparation\n",
    "data = data[data['text'].notna()]\n",
    "data = data[data['title'].notna()]\n",
    "data = data[data['author'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Next, making a function that takes the dataset as a Pandas dataframe \n",
    "and returns the train/validation splits of texts and labels as lists:\n",
    "\"\"\"\n",
    "def prepare_data(df, test_size=0.2, include_title=True, include_author=True):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        text = df['text'].iloc[i]\n",
    "        label = df['label'].iloc[i]\n",
    "        \n",
    "        if include_title:\n",
    "            text = df['title'].iloc[i] + \" - \" + text\n",
    "        if include_author:\n",
    "            text = df['author'].iloc[i] + \" - \" + text\n",
    "        \n",
    "        if text and label in [0,1]:\n",
    "            texts.append(text)\n",
    "            labels.append(label)\n",
    "            \n",
    "    return train_test_split(texts, labels, test_size=test_size)\n",
    "\n",
    "train_texts, valid_texts, train_labels, valid_labels = prepare_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the dataset\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)\n",
    "valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the encoding into a PyTorch datset\n",
    "class NewsGroupsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor([self.labels[idx]])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "# convert tokenize data into torch dataset\n",
    "train_dataset = NewsGroupsDataset(train_encodings, train_labels)\n",
    "valid_dataset = NewsGroupsDataset(valid_encodings, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 420M/420M [02:22<00:00, 3.09MB/s] \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def computer_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    return {'accuracy':acc,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    per_device_train_batch_size=10,  # batch size per device during training\n",
    "    per_device_eval_batch_size=20,   # batch size for evaluation\n",
    "    warmup_steps=100,                # number of warmup steps for learning rate scheduler\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
    "    # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
    "    logging_steps=200,               # log & save weights each logging_steps\n",
    "    save_steps=200,\n",
    "    evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=computer_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14628\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1463\n",
      "  4%|▍         | 65/1463 [18:43<6:27:51, 16.65s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\Github\\python_exam\\notebooks\\transformers.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Github/python_exam/notebooks/transformers.ipynb#ch0000015?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1317\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=1311'>1312</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=1313'>1314</a>\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=1314'>1315</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=1315'>1316</a>\u001b[0m )\n\u001b[1;32m-> <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=1316'>1317</a>\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=1317'>1318</a>\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=1318'>1319</a>\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=1319'>1320</a>\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=1320'>1321</a>\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=1321'>1322</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1554\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=1551'>1552</a>\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=1552'>1553</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=1553'>1554</a>\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=1555'>1556</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=1556'>1557</a>\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=1557'>1558</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=1558'>1559</a>\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=1559'>1560</a>\u001b[0m ):\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=1560'>1561</a>\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=1561'>1562</a>\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2183\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=2179'>2180</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=2181'>2182</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautocast_smart_context_manager():\n\u001b[1;32m-> <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=2182'>2183</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=2184'>2185</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=2185'>2186</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2215\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=2212'>2213</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=2213'>2214</a>\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=2214'>2215</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=2215'>2216</a>\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=2216'>2217</a>\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/trainer.py?line=2217'>2218</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1554\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1545'>1546</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1546'>1547</a>\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1547'>1548</a>\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1548'>1549</a>\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1549'>1550</a>\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1550'>1551</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1551'>1552</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1553'>1554</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1554'>1555</a>\u001b[0m     input_ids,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1555'>1556</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1556'>1557</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1557'>1558</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1558'>1559</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1559'>1560</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1560'>1561</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1561'>1562</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1562'>1563</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1563'>1564</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1565'>1566</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1567'>1568</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1017\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1007'>1008</a>\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1009'>1010</a>\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1010'>1011</a>\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1011'>1012</a>\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1014'>1015</a>\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1015'>1016</a>\u001b[0m )\n\u001b[1;32m-> <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1016'>1017</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1017'>1018</a>\u001b[0m     embedding_output,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1018'>1019</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1019'>1020</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1020'>1021</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1021'>1022</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1022'>1023</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1023'>1024</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1024'>1025</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1025'>1026</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1026'>1027</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1027'>1028</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1028'>1029</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1029'>1030</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:606\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=596'>597</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=597'>598</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=598'>599</a>\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=602'>603</a>\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=603'>604</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=604'>605</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=605'>606</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=606'>607</a>\u001b[0m         hidden_states,\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=607'>608</a>\u001b[0m         attention_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=608'>609</a>\u001b[0m         layer_head_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=609'>610</a>\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=610'>611</a>\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=611'>612</a>\u001b[0m         past_key_value,\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=612'>613</a>\u001b[0m         output_attentions,\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=613'>614</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=615'>616</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=616'>617</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:493\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=480'>481</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=481'>482</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=482'>483</a>\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=489'>490</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=490'>491</a>\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=491'>492</a>\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=492'>493</a>\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=493'>494</a>\u001b[0m         hidden_states,\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=494'>495</a>\u001b[0m         attention_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=495'>496</a>\u001b[0m         head_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=496'>497</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=497'>498</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=498'>499</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=499'>500</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=501'>502</a>\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:423\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=412'>413</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=413'>414</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=414'>415</a>\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=420'>421</a>\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=421'>422</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=422'>423</a>\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=423'>424</a>\u001b[0m         hidden_states,\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=424'>425</a>\u001b[0m         attention_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=425'>426</a>\u001b[0m         head_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=426'>427</a>\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=427'>428</a>\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=428'>429</a>\u001b[0m         past_key_value,\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=429'>430</a>\u001b[0m         output_attentions,\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=430'>431</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=431'>432</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=432'>433</a>\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:311\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=308'>309</a>\u001b[0m     value_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([past_key_value[\u001b[39m1\u001b[39m], value_layer], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=309'>310</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=310'>311</a>\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey(hidden_states))\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=311'>312</a>\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(hidden_states))\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/models/bert/modeling_bert.py?line=313'>314</a>\u001b[0m query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/olive/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the current model after training\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d9f47b65c263a2f9288544ef47d8d998eaca04285220615b645d93bb6e837d0"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
